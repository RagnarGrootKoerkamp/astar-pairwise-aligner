# See the makefile in the root of the repository for instructions.

rule all:
    input: ['table/tools.tsv', 'table/scaling_e.tsv', 'table/scaling_n.tsv']

import itertools
import shutil
import statistics
import pathlib

## LIMITS

# Timeout (cpu-time) in seconds, or 'unlimited'
#TIMEOUT = 'unlimited'
TIMEOUT = '100'
# Memory limit (virtual memory) in KB, or 'unlimited'
MEMORY_LIMIT = 30000000

## TOOL DEFINITIONS
binaries = {
    'pa' : '../target/release/astar-pairwise-aligner',
    'edlib' : '../../edlib/meson-build/edlib-aligner',
    'wfa'   : '../../wfa2/bin/align_benchmark',
}
tool_command = {
    'sh':            binaries["pa"]      + ' -i {input} -o {stats_path} -ss -k {wildcards.k} -r {wildcards.r} --algorithm sh',
    'csh':           binaries["pa"]      + ' -i {input} -o {stats_path} -ss -k {wildcards.k} -r {wildcards.r} --algorithm csh',
    'csh-dt':        binaries["pa"]      + ' -i {input} -o {stats_path} -ss -k {wildcards.k} -r {wildcards.r} --algorithm csh-dt',
    'csh-gap-cost-dt': binaries["pa"]      + ' -i {input} -o {stats_path} -ss -k {wildcards.k} -r {wildcards.r} --algorithm csh-gap-cost-dt',
    # Slower variants for comparison
    'sh-noprune':    binaries["pa"]      + ' -i {input} -o {stats_path} -ss -k {wildcards.k} -r {wildcards.r} --algorithm sh  --no-prune',
    'csh-noprune':   binaries["pa"]      + ' -i {input} -o {stats_path} -ss -k {wildcards.k} -r {wildcards.r} --algorithm csh --no-prune',
    'dijkstra':      binaries["pa"]      + ' -i {input} -o {stats_path} -ss                                   --algorithm dijkstra',
    # Others
    # We use a fork of edlib that takes the .seq files as input and processes pairs one by one.
    'edlib':         binaries["edlib"]   + ' -p -s {input}', # -p: compute the path, -s: no output
    'biwfa':         binaries["wfa" ]    + ' -i {input} -a edit-wfa --wfa-memory-mode ultralow -o {input}.score',
}

# Command to generate testcases.
generate_command    = '../target/release/examples/generate_dataset -x {wildcards.cnt} -n {wildcards.n} -e {wildcards.e} {output}'

# Wrap `command` in a small shell script than handles timeouts and memory limits
# using `ulimit`, and write time and memory usage to `bench_path`.
def timeout(command, bench_path):
    return f"""
# Disable coredumps and set the time and memory limit
ulimit -c 0
ulimit -t {TIMEOUT}
ulimit -v {MEMORY_LIMIT}
# Allow the command to fail.
export TIME="s\tuser\tsystem\tmax_rss\texit_status\n%e\t%U\t%S\t%M\t%x"
set +e
/usr/bin/time -o {bench_path} {command}
status=$?
set -e
# NOTE: time returns 0 when the process was killed by a signal.
# In this case, the first line of the bench_path is "Command terminated by signal <signal>"
# We handle this separately.
start=$(head -c 7 {bench_path})
if [ "$start" == "Command" ] ; then
    status=$(head -1 {bench_path} | cut -d ' ' -f 5)
    sed -i '1d' {bench_path}
    sed -i "s/0$/$status/" {bench_path}
fi
# Normal exit
if [ $status -eq 0 ] ; then
    exit 0
fi
# Out of memory from SIGKILL (9) (when killed by linux) or SIGSEGV (11) or SIGABRT (6)
if [ $status -eq 9 ] || [ $status -eq 11 ] || [ $status -eq 6 ]; then
    exit 0
fi
echo Unexpected exit status $status
exit 1
"""

wildcard_constraints:
    # Error rates start with a digit.
    e="[0-9.]+",
    # Tool names do not start with a digit.
    tool='[^.0-9]*'

## WRAPPER CLASSES

# An input is a generated file containing test sequences.
class RandomInput:
    # An input dataset is given by:
    def __init__(self, **kwargs):
        # The number of sequence pairs.
        self.cnt = int(kwargs['cnt'])
        # The length of each sequence.
        self.n = int(kwargs['n'])
        # The error rate.
        self.e = float(kwargs['e'])

    # The wildcard string used for pattern matches.
    def name_pattern():
        return 'x{cnt}-n{n}-e{e}'
    # The wildcard pattern for an input file.
    def pattern():
        return f'input/synthetic/{RandomInput.name_pattern()}.seq'

    # The name of this input file.
    def name(self):
        return Path(f'x{self.cnt}-n{self.n}-e{self.e}')
    # The path of this input file.
    def path(self):
        return Path(f'input/synthetic/{self.name()}.seq')

    def table_headers(self):
        return ["cnt", "n", "e"]
    def table_cells(self):
        return [self.cnt, self.n, self.e]

class HumanInput:
    # An input dataset is given by:
    def __init__(self, **kwargs):
        p = pathlib.Path(kwargs['path'])
        if p.parts[0] == 'human':
            p = pathlib.Path(*p.parts[1:])
        if p.suffix == '.seq':
            p = p.with_suffix('')
        self.dir = p.parts[0]
        self.id = str(pathlib.Path(*p.parts[1:]))
        assert self.id.startswith('seq')
        self.id = self.id.lstrip('seq')

    # The wildcard string used for pattern matches.
    def name_pattern():
        return '{path}'
    # The wildcard pattern for an input file.
    def pattern():
        return f'{HumanInput.name_pattern()}.seq'

    # The name of this input file.
    def name(self):
        return Path(f'{self.dir}/seq{self.id}')
    # The path of this input file.
    def path(self):
        return Path(f'human/{self.name()}.seq')

    def table_headers(self):
        return ["dir", "id"]
    def table_cells(self):
        return [self.dir, self.id]


# A run is an execution of a tool (with optional parameters) on an input.
# The most important part is the bench_path, which is where Snakemake will write the timings.
class Run:
    # A run consists of an input and tool, and optionally r and k parameters.
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)
        self.has_params = 'k' in kwargs and 'r' in kwargs
    # The wildcard pattern for an run benchmark with parameters.
    def pattern_with_params(input):
        return f'runs/{input}-A{{tool}}-r{{r}}-k{{k}}.bench'
    # The wildcard pattern for an run benchmark without parameters.
    def pattern(input):
        return f'runs/{input}-A{{tool}}.bench'
    # The benchmark path of this run, containing the time and memory usage measured by `/usr/bin/time`.
    def bench_path(self):
        if self.has_params:
            return Path(f'runs/{self.input.name()}-A{self.tool}-r{self.r}-k{self.k}.bench')
        else:
            return Path(f'runs/{self.input.name()}-A{self.tool}.bench')
    # The stats path of this run, where A*PA writes additional statistics.
    def stats_path(self):
        return self.bench_path().with_suffix('.stats')

# === GENERATE INPUT ===

rule generate_input:
    # Input generation has priority over other rules, so that all inputs are
    # generated at the start.
    priority: 2
    output: RandomInput.pattern()
    shell: generate_command

# === RULES TO RUN ON TESTCASES ===

# Given a set of wildcards (input specs, tool, parameters), return the command to run.
def build_command(w):
    if 'path' in w.keys():
        input = HumanInput(**w)
    else:
        input = RandomInput(**w)
    run = Run(input=input, **w)
    command = tool_command[w.tool].format(input=input.path(), wildcards=w, stats_path=run.stats_path())
    return timeout(command, run.bench_path())

# Benchmark the tool.
rule benchmark_tool:
    input: lambda w: RandomInput(**w).path()
    output: Run.pattern(RandomInput.name_pattern())
    params: command = lambda w: build_command(w)
    shell: '{params.command}'

# Benchmark the tool with given parameters.
rule benchmark_tool_with_params:
    input: lambda w: RandomInput(**w).path()
    output: Run.pattern_with_params(RandomInput.name_pattern())
    params: command = lambda w: build_command(w)
    shell: '{params.command}'

# Benchmark the tool.
rule benchmark_tool_human:
    input: lambda w: HumanInput(**w).path()
    output: Run.pattern(HumanInput.name_pattern())
    params: command = lambda w: build_command(w)
    shell: '{params.command}'

# Benchmark the tool with given parameters.
rule benchmark_tool_human_with_params:
    input: lambda w: HumanInput(**w).path()
    output: Run.pattern_with_params(HumanInput.name_pattern())
    params: command = lambda w: build_command(w)
    shell: '{params.command}'

# === TABLE CONSTRUCTION ===

# Collect all run files into a single tsv:
# - input parameters
# - .bench file
# - .status file
# - .stats file if present
def make_table(runs, output):
    headers = ["alg"] + runs[0].input.table_headers() + ["r","k","s","user","system","max_rss","exit_status", "ed"]
    table_file = Path(output[0]).open('w')

    found_headers = False
    for run in runs:
        if run.stats_path().is_file():
            pa_headers = run.stats_path().read_text().splitlines()[0]
            table_file.write("\t".join(headers) + '\t' + pa_headers + '\n')
            found_headers = True
            break
    if not found_headers:
        table_file.write("\t".join(headers) + '\n')

    for run in runs:
        try:
            score_file = run.input.path().parent / (run.input.path().name + '.score')
            ed = score_file.read_text().split()[0]
        except:
            ed = 0
        try:
            stats = run.stats_path().read_text().splitlines()[1]
        except:
            stats = ''
        if run.bench_path().is_file():
            for bench_line in run.bench_path().read_text().splitlines()[1:]:
                input_fields = '\t'.join(map(str, run.input.table_cells()))
                table_file.write(f'{run.tool}\t{input_fields}\t{getattr(run, "r", "")}\t{getattr(run, "k", "")}\t{bench_line}\t{ed}\t{stats}\n')

# === TOOL COMPARISON ===

# Returns the list of runs for the tool comparison table.
def tools_runs():
    N = 10**8
    ns = [3_000, 10_000, 30_000, 100_000, 300_000, 1_000_000, 3_000_000, 10_000_000]
    es = [0.01, 0.05, 0.10, 0.15]
    tools = ['edlib', 'biwfa', 'sh', 'csh']
    inputs = [RandomInput(cnt=N//n, n=n, e=e) for n in ns for e in es if N//n > 0]
    return [
        (
            # NOTE: exact matches are used for e < 10%, and inexact matches for e >= 10%.
            Run(input=input,
                tool=tool,
                k=15,
                r=1 if input.e < 0.1 else 2)
            if tool in ['sh', 'csh'] else
            Run(input=input, tool=tool)
        )
        for input in inputs
        for tool in tools
    ]

rule tools_table:
    priority: 2
    input: lambda w: [run.bench_path() for run in tools_runs()]
    output: 'table/tools.tsv'
    run: make_table(tools_runs(), output)

# === SCALING WITH N ===

# Returns the list of runs for the scaling with n comparison with and without pruning.
def scaling_n_runs():
    ns = [100, 300, 1000, 3000, 10_000, 30_000, 100_000]
    e = 0.05
    r = 1
    k = 15
    # Split tools into two groups using a different N.
    fast_N = 10**8
    slow_N = 10**6
    fast_tools = ['sh', 'csh']
    slow_tools = ['sh-noprune', 'csh-noprune']
    fast_inputs = [RandomInput(cnt=fast_N//n, n=n, e=e) for n in ns if fast_N//n > 0]
    slow_inputs = [RandomInput(cnt=slow_N//n, n=n, e=e) for n in ns if slow_N//n > 0]
    # Dijkstra is separate because it does not use parameters.
    dijkstra_runs = [Run(input=input, tool=tool)
                        for input in slow_inputs
                        for tool in ['dijkstra']]
    fast_runs = [Run(input=input, tool=tool, r=r, k=k)
                        for input in fast_inputs
                        for tool in fast_tools]
    slow_runs = [Run(input=input, tool=tool, r=r, k=k)
                        for input in slow_inputs
                        for tool in slow_tools]
    return fast_runs + slow_runs + dijkstra_runs

rule scaling_n_table:
    priority: 2
    input: lambda w: [run.bench_path() for run in scaling_n_runs()]
    output: 'table/scaling_n.tsv'
    run: make_table(scaling_n_runs(), output)

# === SCALING WITH E ===

# Returns the list of runs for scaling with e, with exact and inexact matches.
def scaling_e_runs():
    N = 10**6
    n = 10000
    es = [0.01] + [x/100 for x in range(2, 35, 2)]
    rs = [1, 2]
    k = 9
    tools = ['sh', 'csh']

    inputs = [RandomInput(cnt=N//n, n=n, e=e) for e in es if N//n > 0]
    # Dijkstra is separate because it does not use parameters.
    dijkstra_runs = [Run(input=input, tool=tool)
                        for input in inputs
                        for tool in ['dijkstra']]
    our_runs = [Run(input=input, tool=tool, r=r, k=k)
                        for r in rs
                        for input in inputs
                        for tool in tools]
    return dijkstra_runs + our_runs

rule scaling_e_table:
    priority: 2
    input: lambda w: [run.bench_path() for run in scaling_e_runs()]
    output: 'table/scaling_e.tsv'
    run: make_table(scaling_e_runs(), output)

# =========================== HUMAN DATA ============================

# Comparison on 500k+ ONT-UL MinION reads (from BiWFA paper)
# and 500k+ ONT reads from CHM13.

# Returns the list of runs for the tool comparison table.
def human_runs(dir):
    tools = ['edlib', 'biwfa', 'sh', 'csh']
    paths = sorted(pathlib.Path('human').glob(f'{dir}/seq*.seq'))
    inputs = [HumanInput(path=path) for path in paths]
    rks = [(2, 15)]
    return [
        (
            Run(input=input, tool=tool, r=r, k=k)
            if 'sh' in tool else
            Run(input=input, tool=tool)
        )
        for input in inputs
        for tool in tools
        for (r, k) in (rks if 'sh' in tool else [(0,0)])
    ]

rule ont_ul_table:
    priority: 2
    input: lambda w: [run.bench_path() for run in human_runs('ont-ul')]
    output: 'table/human_ont-ul.tsv'
    run: make_table(human_runs('ont-ul'), output)

rule chm13_table:
    priority: 2
    input: lambda w: [run.bench_path() for run in human_runs('chm13')]
    output: 'table/human_chm13.tsv'
    run: make_table(human_runs('chm13'), output)

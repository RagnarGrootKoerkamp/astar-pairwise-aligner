# Instructions for compiling:
# - A* Pairwise Aligner:
#   - cargo build --release
# - edlib:
#   - From this directory:
#   - cd ..
#   - Clone the fork, which contains an updated binary to handle the WFA input format.
#   - git clone https://github.com/RagnarGrootKoerkamp/edlib
#   - cd edlib/build
#   - cmake -D CMAKE_BUILD_TYPE=Release .. && make
# - WFA:
#   - From this directory:
#   - cd ..
#   - Clone the fork, which contains a fix to allow setting parameters to mimick edit distance.
#   - git clone https://github.com/RagnarGrootKoerkamp/WFA wfa
#   - cd wfa
#   - make clean all
#
# Instructions for running:
# - snakemake -p -c all run_benchmark

import itertools
import shutil
import statistics

## TOOLS

## RUN SETTINGS
# Time after which snakemake kills the program.
TIMEOUT = "7200s"
# Time passed to the program.
TOOL_TIMEOUT = "8000s"
REPEATS = 1

## TOOL DEFINITIONS
binaries = {
    'pa' : '../target/release/pairwise-aligner',
    'edlib' : '../../edlib/meson-build/edlib-aligner',
    'wfa'   : '../../wfa/bin/align_benchmark',
    'wfa2'   : '../../wfa2/bin/align_benchmark',
    'biwfa'   : '../../biwfa/bin/align_benchmark',
}
tool_command = {
    # Versions with manual parameter choosing.
    'cp-sh':            binaries["pa"]      + ' -i {input} -o {stats_path} --silent2 --timeout {timeout} -k {wildcards.k} -m {wildcards.m} --algorithm SH',
    'cp-csh':           binaries["pa"]      + ' -i {input} -o {stats_path} --silent2 --timeout {timeout} -k {wildcards.k} -m {wildcards.m} --algorithm CSH',
    'cp-csh+gap':       binaries["pa"]      + ' -i {input} -o {stats_path} --silent2 --timeout {timeout} -k {wildcards.k} -m {wildcards.m} --algorithm CSH_GapCost',
    'cp-sh-noprune':       binaries["pa"]      + ' -i {input} -o {stats_path} --silent2 --timeout {timeout} -k {wildcards.k} -m {wildcards.m} --no-prune --algorithm SH',
    'cp-csh-noprune':      binaries["pa"]      + ' -i {input} -o {stats_path} --silent2 --timeout {timeout} -k {wildcards.k} -m {wildcards.m} --no-prune --algorithm CSH',
    'cp-csh+gap-noprune':  binaries["pa"]      + ' -i {input} -o {stats_path} --silent2 --timeout {timeout} -k {wildcards.k} -m {wildcards.m} --no-prune --algorithm CSH_GapCost',
    # choosed k and m values automatically.
    'sh':               binaries["pa"]      + ' -i {input} -o {stats_path} --silent2 --timeout {timeout} --error-rate {e} --algorithm SH',
    'csh':              binaries["pa"]      + ' -i {input} -o {stats_path} --silent2 --timeout {timeout} --error-rate {e} --algorithm CSH',
    'csh+gap':          binaries["pa"]      + ' -i {input} -o {stats_path} --silent2 --timeout {timeout} --error-rate {e} --algorithm CSH_GapCost',
    # slower PA variants
    'dijkstra':         binaries["pa"]      + ' -i {input} -o {stats_path} --silent2 --timeout {timeout} --error-rate {e} --algorithm Dijkstra',
    'sh-noprune':       binaries["pa"]      + ' -i {input} -o {stats_path} --silent2 --timeout {timeout} --error-rate {e} --no-prune --algorithm SH',
    'csh-noprune':      binaries["pa"]      + ' -i {input} -o {stats_path} --silent2 --timeout {timeout} --error-rate {e} --no-prune --algorithm CSH',
    'csh+gap-noprune':  binaries["pa"]      + ' -i {input} -o {stats_path} --silent2 --timeout {timeout} --error-rate {e} --no-prune --algorithm CSH_GapCost',
    # Others
    # We use a fork of edlib that takes the .seq files as input and processes pairs one by one.
    'edlib':            binaries["edlib"]   + '    {input} -p -s', # -p: compute the path, -s: no output
    'wfa':              binaries["wfa"]     + ' -i {input} -a gap-affine-wfa --affine-penalties="0,1,0,1"',
    'wfa2':             binaries["wfa2"]    + ' -i {input} -a edit-wfa',
    'biwfa':            binaries["biwfa"]   + ' -i {input} --affine-penalties 0,1,0,1 --wfa-bidirectional',
}

# Generate testcases
generate_command    = '../target/release/examples/generate_dataset -x {wildcards.cnt} -n {wildcards.n} -e {wildcards.e} -o {output}'

# Wrap a {command} in a small shell script than handles timeouts.
# Uses the TIMEOUT variable, and writes status to params.status_path.
def timeout(command, status_path, timeout):
    return f"""
# Allow this command to fail.
set +e
timeout {timeout} {command}
exit_code=$?
set -e
# Normal exit
if [ $exit_code -eq 0 ] ; then
    echo ok > {status_path}
    exit 0
fi
if [ $exit_code -eq 124 ] ; then
    echo timeout {timeout} > {status_path}
    exit 0
fi
# Handle out of memory (OOM);
# typically from SIGKILL (137) (when killed by linux) or SIGSEGV (139) or SIGABRT (134)
if [ $exit_code -eq 137 ] || [ $exit_code -eq 139 ] || [ $exit_code -eq 134 ]; then
    echo out-of-memory $exit_code > {status_path}
    exit 0
fi
echo Unexpected exit status $exit_code
# Fail with an error.
exit 1
"""


## WRAPPER CLASSES
wildcard_constraints:
    e="[0-9.]+",
    tool='[^.0-9]*'


class Input:
    def __init__(self, **kwargs):
        self.cnt = int(kwargs['cnt'])
        self.n = int(kwargs['n'])
        self.ont = 'model' in kwargs
        if self.ont:
            self.model = kwargs['model']
            # ONT has roughly 10% error rate
            self.e = 0.1
        else:
            self.model = 'synthetic'
            self.e = float(kwargs['e'])
    def name_pattern(ont=False):
        if ont:
            return '{model}-x{cnt}-n{n}'
        else:
            return 'x{cnt}-n{n}-e{e}'
    def dir(ont=False):
        if ont:
            return 'simulated'
        else:
            return 'synthetic'
    def pattern(ont=False):
        if ont:
            return f'input/{Input.dir(ont)}/{Input.name_pattern(ont)}.seq'
        else:
            return f'input/{Input.dir(ont)}/{Input.name_pattern(ont)}.seq'
    def name(self):
        if self.ont:
            return Path(f'{self.model}-x{self.cnt}-n{self.n}')
        else:
            return Path(f'x{self.cnt}-n{self.n}-e{self.e}')
    def path(self):
        return Path(f'input/{Input.dir(self.ont)}/{self.name()}.seq')


class Run:
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)
        self.has_params = 'k' in kwargs and 'm' in kwargs
    def pattern_with_params(ont=False):
        return f'runs/{Input.name_pattern(ont)}-A{{tool}}-m{{m}}-k{{k}}.bench'
    def pattern(ont=False):
        return f'runs/{Input.name_pattern(ont)}-A{{tool}}.bench'
    def bench_path(self):
        if self.has_params:
            return Path(f'runs/{self.input.name()}-A{self.tool}-m{self.m}-k{self.k}.bench')
        else:
            return Path(f'runs/{self.input.name()}-A{self.tool}.bench')
    def status_path(self):
        return self.bench_path().with_suffix('.status')
    def stats_path(self):
        return self.bench_path().with_suffix('.stats')

# Returns false for parameters that should be skipped.
def params_filter(run):
    assert run.has_params

    # If the error rate is larger than the max rate this (k,m) can handle, skip.
    # e is made a bit lower, since an induced error rate of 20% only gives ~15% edit distance.
    max_error_rate = (run.m+1) / (run.k-1) # -1 to allow slightly too large k.
    if run.input.e * 0.85 > max_error_rate:
        return False

    # Don't do inexact matches for low error rates.
    if run.input.e < 0.10 and run.m > 0:
        return False

    # If we expect multiple matches per seed, skip since we should increase k instead.
    if run.m == 0 and run.input.n / 4.**(run.k+2) > 2*(run.k+1): # +1 to allow slightly too small k.
        return False
    # The number of distance-1 matches per seed is roughly 8*k.
    if run.m == 1 and 8*(run.k+1)* run.input.n / 4.**(run.k+2) > 2*(run.k+2): # +2 allow slightly too small k.
        return False

    return True

# === GENERATE INPUT ===

rule generate_input:
    output: Input.pattern()
    shell: generate_command

# === RULES TO RUN ON TESTCASES ===

def build_command(w):
    input = Input(**w)
    run = Run(input=input, **w)
    command = tool_command[w.tool].format(input=input.path(), e=input.e, wildcards=w, status_path=run.status_path(), stats_path=run.stats_path(), timeout=TOOL_TIMEOUT)
    return timeout(command, run.status_path(), TIMEOUT)

rule run_tool:
    input: lambda w: Input(**w).path()
    benchmark: repeat(Run.pattern(), REPEATS)
    params: command = lambda w: build_command(w)
    shell: "{params.command}"

rule run_tool_with_parameters:
    input: lambda w: Input(**w).path()
    benchmark: repeat(Run.pattern_with_params(), REPEATS)
    wildcard_constraints: tool = '.*cp-.*'
    params: command = lambda w: build_command(w)
    shell: '{params.command}'

# === TABLE CONSTRUCTION ===

# Collect all run files into a single tsv:
# - input parameters
# - .bench file
# - .status file
# - .stats file if present
def make_table(runs, output):
    headers = ["alg","model", "cnt","n","e","m","k","s","h:m:s","max_rss","max_vms","max_uss","max_pss","io_in","io_out","mean_load","cpu_time", "exit_status"]
    table_file = Path(output[0]).open('w')

    found_headers = False
    for run in runs:
        if run.stats_path().is_file():
            pa_headers = run.stats_path().read_text().splitlines()[0]
            table_file.write("\t".join(headers) + '\t' + pa_headers + '\n')
            found_headers = True
            break
    if not found_headers:
        table_file.write("\t".join(headers) + '\n')

    for run in runs:
        status = run.status_path().read_text().strip()
        try:
            stats = '\t' + run.stats_path().read_text().splitlines()[1]
        except:
            stats = ''
        for bench_line in run.bench_path().read_text().splitlines()[1:]:
            table_file.write(f'{run.tool}\t{run.input.model}\t{run.input.cnt}\t{run.input.n}\t{run.input.e}\t{getattr(run, "m", "")}\t{getattr(run, "k", "")}\t{bench_line}\t{status}{stats}\n')

# === PARAMETER COMPARISON ===

def params_runs(N):
    N = int(float(N))
    ns = [100_000, 1_000_000, 3_000_000, 10_000_000]
    es = [0.10, 0.15]
    mks = [
       #(0,7), (0,8), (9,0), (0, 10), (0, 11), (0, 12), (0,13), (0,14), (0,15), (0,16), (0, 17), (0, 31),
       (1, 14), (1, 15), (1, 16)]
    tools = ['cp-sh', 'cp-csh']
    inputs = [Input(cnt=N//n, n=n, e=e) for n in ns for e in es if N//n > 0]
    return [Run(input=input, tool=tool, m=m, k=k)
                        for (m, k) in mks
                        for input in inputs
                        for tool in tools]
rule params_table:
    input: lambda w: [run.bench_path() for run in params_runs(w.N)]
    output: 'table/params_N{N}.tsv'
    run: make_table(params_runs(wildcards.N), output)

# === TOOL COMPARISON ===

def tools_runs(N):
    N = int(float(N))
    ns = [100, 300, 1_000, 3_000, 10_000, 30_000, 100_000, 300_000, 1_000_000, 3_000_000, 10_000_000]
    es = [0.01, 0.05, 0.10, 0.15]
    tools = ['edlib', 'biwfa', 'sh', 'csh']
    inputs = [Input(cnt=N//n, n=n, e=e) for n in ns for e in es if N//n > 0]
    fast_runs = [Run(input=input, tool=tool)
                        for input in inputs
                        for tool in tools]

    # Change range for slower tools.
    N //= 100
    tools= [
        'dijkstra',
        'csh-noprune',
    ]
    inputs = [Input(cnt=N//n, n=n, e=e) for n in ns for e in es if N//n > 0]
    slow_runs = [Run(input=input, tool=tool)
                        for input in inputs
                        for tool in tools]
    return fast_runs + slow_runs
rule tools_table:
    input: lambda w: [run.bench_path() for run in tools_runs(w.N)]
    output: 'table/tools_N{N}.tsv'
    run: make_table(tools_runs(wildcards.N), output)

# === SCALING WITH E ===

def scaling_e_runs(N):
    N = int(float(N))
    ns = [10000]
    es = [0.01] + [x/100 for x in range(2, 40, 2)]
    ms = [0, 1]
    ks = [11]
    tools = ['cp-sh', 'cp-csh', 'cp-csh+gap']

    inputs = [Input(cnt=N//n, n=n, e=e) for n in ns for e in es if N//n > 0]
    # Dijkstra is separate because it does not use parameters.
    dijkstra_runs = [Run(input=input, tool=tool)
                        for input in inputs
                        for tool in ['dijkstra']]
    our_runs = [Run(input=input, tool=tool, m=m, k=k)
                        for k in ks
                        for m in ms
                        for input in inputs
                        for tool in tools]
    return dijkstra_runs + our_runs

rule scaling_e_table:
    input: lambda w: [run.bench_path() for run in scaling_e_runs(w.N)]
    output: 'table/scaling_e_N{N}.tsv'
    run: make_table(scaling_e_runs(wildcards.N), output)

# === SCALING WITH N ===

def scaling_n_runs(N):
    N = int(float(N))
    ns = [100, 300, 1000, 3000, 10_000, 30_000, 100_000, 300_000, 1_000_000]
    es = [0.01]
    ms = [0]
    ks = [15]
    tools = ['cp-sh-noprune', 'cp-csh-noprune', 'cp-sh', 'cp-csh']
    inputs = [Input(cnt=N//n, n=n, e=e) for n in ns for e in es if N//n > 0]
    # Dijkstra is separate because it does not use parameters.
    dijkstra_runs = [Run(input=input, tool=tool)
                        for input in inputs
                        for tool in ['dijkstra']]
    our_runs = [Run(input=input, tool=tool, m=m, k=k)
                        for k in ks
                        for m in ms
                        for input in inputs
                        for tool in tools]
    return dijkstra_runs + our_runs

rule scaling_n_table:
    input: lambda w: [run.bench_path() for run in scaling_n_runs(w.N)]
    output: 'table/scaling_n_N{N}.tsv'
    run: make_table(scaling_n_runs(wildcards.N), output)

# === SCALING WITH K (vary n) ===

def scaling_k_runs(N):
    N = int(float(N))
    ns = [100, 300, 1000, 3000, 10_000, 30_000, 100_000, 300_000, 1_000_000, 3_000_000, 10_000_000]
    es = [0.05]
    ms = [0]
    ks = range(5, 26, 1)
    tools = ['cp-sh', 'cp-csh']
    inputs = [Input(cnt=N//n, n=n, e=e) for n in ns for e in es if N//n > 0]
    return [Run(input=input, tool=tool, m=m, k=k)
                        for k in ks
                        for m in ms
                        for input in inputs
                        for tool in tools]

rule scaling_k_table:
    input: lambda w: [run.bench_path() for run in scaling_k_runs(w.N)]
    output: 'table/scaling_k_N{N}.tsv'
    run: make_table(scaling_k_runs(wildcards.N), output)

# === SCALING WITH K (vary e) ===

def scaling_k_e_runs(N):
    N = int(float(N))
    ns = [10000]
    es = [0.01] + [x/100 for x in range(2, 15, 2)]
    ms = [0]
    ks = range(5, 26, 1)
    tools = ['cp-sh', 'cp-csh']
    inputs = [Input(cnt=N//n, n=n, e=e) for n in ns for e in es if N//n > 0]
    return [Run(input=input, tool=tool, m=m, k=k)
                        for k in ks
                        for m in ms
                        for input in inputs
                        for tool in tools]

rule scaling_k_e_table:
    input: lambda w: [run.bench_path() for run in scaling_k_e_runs(w.N)]
    output: 'table/scaling_k_e_N{N}.tsv'
    run: make_table(scaling_k_e_runs(wildcards.N), output)


# ==== ONT Runs ==== #

ont_ns = [100_000, 300_000, 1_000_000]
ont_models = ['random', 'human']
ont_kms = [(14, 1), (15, 1), (16, 1), (17, 1)]

# This runs on 3 types of data:
# - synthetic data
# - random sequences with ONT errors
# - human sequences with ONT errors
def ont_inputs(N):
    N = int(float(N))
    return ([Input(cnt=N//n, n=n, e=0.1) for n in ont_ns if N//n > 0]
            + [Input(model=model, cnt=N//n, n=n) for n in ont_ns for model in ont_models if N//n > 0])

# Run PA with given parameters.
rule ont_run_pa_for_parameters:
    input: lambda w: Input(**w).path()
    benchmark: repeat(Run.pattern_with_params(ont=True), REPEATS)
    wildcard_constraints: tool = '.*cp-.*'
    params: command = lambda w: build_command(w, params=True)
    shell: '{params.command}'

rule ont_run_tool:
    input: lambda w: Input(**w).path()
    benchmark: repeat(Run.pattern(ont=True), REPEATS)
    params: command = lambda w: build_command(w, params=False)
    shell: "{params.command}"

rule ont_params_table:
    input: lambda w: [run.bench_path() for run in params_runs(tools_cp, ont_inputs(w.N), ont=True)]
    output: 'table/params_ont_N{N}.tsv'
    run: make_table(params_runs(tools_cp, ont_inputs(wildcards.N), ont=True), output)

rule ont_tools_table:
    input: lambda w: [run.bench_path() for run in tool_runs(tools, ont_inputs(w.N))]
    output: 'table/tools_ont_N{N}.tsv'
    run: make_table(tool_runs(tools, ont_inputs(wildcards.N)), output)


# ==== Visualizations ==== #

rule astar_visualization:
    input:
    output: 'astar-visualization/dijkstra.csv'
    # Note: no --release, since this data is only tracked in debug mode.
    shell: "cargo run --example astar_visualization"

# ======= Reference ======= #
rule human_reference:
    #output: f'input/reference/human.fa'
    shell: '''
    wget -P input/reference http://ftp.ensembl.org/pub/release-106/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz
    gunzip --keep input/reference/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz {output}
    # Make .fai
    samtools faidx {output}
    '''

# ======= Nanosim data ======= #

rule nanosim:
    output: 'input/simulated/{ref}-x{x}-n{n}.seq'
    input: 'input/reference/{ref}.fa'
    threads: 8
    params:
        min = lambda w: int(w.n)*3//4,
        max = lambda w: int(w.n)*5//4,
        seed = 31415,
        # Generate a few more sequences, so that we have some room for filtering (unaligned, and N regions).
        generate_x = lambda w: int(w.x) * 3 // 2
    # NOTE: Run this in a conda env with nanosim loaded.
    # TODO: Add this info to the snakefile.
    # TODO: Fix path to the nanosim datafiles.
    shell: '''
    # Run the simulator to generate a .fa file.
    simulator.py genome \
      --ref_g input/reference/{wildcards.ref}.fa \
      --output input/simulated/{wildcards.ref}-x{wildcards.x}-n{wildcards.n} \
      -dna_type linear \
      --model_prefix ../../nanosim/pre-trained_models/human_NA12878_DNA_FAB49712_guppy/training \
      --min_len {params.min} \
      --median_len {wildcards.n} \
      --max_len {params.max} \
      --sd_len 1.05 \
      --number {params.generate_x} \
      --strandness 1 \
      --seed 314151 \
      --num_threads 8
    # Convert to a .seq file,
    ../target/release/examples/nanosim_to_seq \
      --reference input/reference/{wildcards.ref}.fa \
      --reads input/simulated/{wildcards.ref}-x{wildcards.x}-n{wildcards.n}_aligned_reads.fasta \
      --output input/simulated/{wildcards.ref}-x{wildcards.x}-n{wildcards.n}.seq \
      --count {wildcards.x} \
      --strip-unaligned
    '''

#
# simulator.py genome
# -rg ../reads/reference/GRCh38_full_analysis_set_plus_decoy_hla.fa
# -dna_type linear
# -c ../../../../nanosim/pre-trained_models/human_NA12878_DNA_FAB49712_guppy/training
# -min  750000
# -max 12500000
# -med 1000000
# -sd 1.05
# -n10
# -s 1
# --seed 31415




# ================ BELOW HERE IS unused AND untested REAL DATA =================== #

# Nanopore / Illumina data is here: https://github.com/Daniel-Liu-c0deb0t/block-aligner/releases

# ONT long reads
# Block aligner uses data from the
# Whole Human Genome Sequencing Project
# Data sources: https://github.com/nanopore-wgs-consortium/NA12878/blob/master/nanopore-human-genome/rel_3_4.md

# Download reference genome.
# Gets the entire directory, but not subdirectories.
REFERENCE_FTP = 'ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/GRCh38_reference_genome/'
REFERENCE2_DIR = 'input/reads/reference'
REFERENCE_NAME = f'{REFERENCE2_DIR}/GRCh38_full_analysis_set_plus_decoy_hla.fa'
rule download_reference:
    output: REFERENCE_NAME
    shell: f'wget -r -nH -nc -l 1 -P {REFERENCE2_DIR} --cut-dirs=5 {REFERENCE_FTP}'


# Download reads
# [(flowcel id, fasta url, bam url, bai url)]
FLOWCELLS = {
    'FAB45271': 'nanopore-human-wgs/rel3-nanopore-wgs-152889212-FAB45271.fastq.gz',
    'FAB42316': 'nanopore-human-wgs/rel3-nanopore-wgs-216722908-FAB42316.fastq.gz',
    'FAB49164': 'nanopore-human-wgs/rel3-nanopore-wgs-4045668814-FAB49164.fastq.gz',
}

# Copy from AWS S3.
# Using AWS CLI is faster, but needs an AWS account and aws CLI set up and linked to your account.
rule read:
    output: 'input/reads/{id}.fastq.gz'
    run:
        path = FLOWCELLS[wildcards.id]
        #shell('wget -O {output} http://s3.amazonaws.com/{path}')
        shell('aws configure set default.s3.max_concurrent_requests 20')
        shell('aws s3 cp s3://{path} {output}')

rule read_bam:
    output: 'input/reads/{id}.fastq.gz.sorted.bam'
    run:
        path = FLOWCELLS[wildcards.id] + '.sorted.bam'
        for ext in ['', '.bai']:
            #shell('wget -O {output}{ext} http://s3.amazonaws.com/{path}{ext}')
            shell('aws configure set default.s3.max_concurrent_requests 20')
            shell('aws s3 cp s3://{path}{ext} {output}{ext}')

rule download_reads:
    input: [f'input/reads/{id}.fastq.gz{ext}' for ext in ['', '.sorted.bam'] for id in FLOWCELLS]

# Make .bed file containing reference locations of reads in .bam file:
rule bed_file:
    input: 'input/reads/{id}.fastq.gz.sorted.bam'
    output: 'input/reads/{id}.bed'
    shell: 'bedtools bamtobed -i input/reads{id}.fastq.gz.sorted.bam > {output}'

# Extract parts of reference corresponding to reads:
rule extract_reference:
    input: ['input/reads/{id}.bed', REFERENCE_NAME]
    output: 'input/reads/{id}.mapped.fa'
    # -s: reverse-complement the reference when needed
    shell: 'bedtools getfasta -s -nameOnly -fi {REFERENCE_NAME}  -bed input/reads/{id}.bed > {output}'

# Reads are in the original .fasta.gz file, or can be extracted using
# samtools fasta {id}.fastq.gz.sorted.bam
# View the bam entry for a read with
# samtools view {read_id}

# Large (500k+) read set from BiWFA paper:
rule download_ont_500k:
    output: 'input/downloads/ont_500k.zip'
    shell: 'wget -O {output} https://github.com/smarco/BiWFA-paper/blob/main/evaluation/data/ONT_MinION_UL.500kbps.zip?raw=true'
rule ont_500k:
    input: 'input/downloads/ont_500k.zip'
    output: 'input/ont_500k/all.seq'
    params: dir = "input/ont_500k/"
    shell: '''
    mkdir -p input/ont_500k/
    unzip {input} -d {params.dir}
    cat {params.dir}/* > {output}
    '''

# Instructions for compiling:
# - A* Pairwise Aligner:
#   - cargo build --release
# - edlib:
#   - From this directory:
#   - cd ..
#   - Clone the fork, which contains an updated binary to handle the WFA input format.
#   - git clone https://github.com/RagnarGrootKoerkamp/edlib
#   - cd edlib/build
#   - cmake -D CMAKE_BUILD_TYPE=Release .. && make
# - WFA:
#   - From this directory:
#   - cd ..
#   - Clone the fork, which contains a fix to allow setting parameters to mimick edit distance.
#   - git clone https://github.com/RagnarGrootKoerkamp/WFA wfa
#   - cd wfa
#   - make clean all
#
# Instructions for running:
# - snakemake -p -c all run_benchmark

import itertools
import shutil
import statistics

## SYNTHETIC INPUT DATA

# Length of each sequence
ns = [100, 300, 1_000, 3_000, 10_000, 30_000, 100_000, 300_000, 1_000_000, 3_000_000, 10_000_000, 30_000_000, 100_000_000]
# Error rate in [0;1]
es = [0.01, 0.05, 0.10, 0.20]

## PA PARAMS

# Seed length, match distance pairs
kms = [(7,0), (8,0), (9,0), (10, 0), (11, 0), (12, 0), (13,0), (14,0), (15,0), (16,0), (17, 0), (31, 0),
       (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]

## TOOLS

## RUN SETTINGS
TIMEOUT = "600s"
REPEATS = 1

## TOOL DEFINITIONS
binaries = {
    'pa' : '../target/release/pairwise-aligner',
    'edlib' : '../../edlib/build/bin/edlib-aligner',
    'wfa'   : '../../wfa/bin/align_benchmark',
    'wfa2'   : '../../wfa2/bin/align_benchmark',
    'biwfa'   : '../../biwfa/bin/align_benchmark',
}
tool_command = {
    # Version with manual parameter choosing.
    'pa': binaries["pa"] + ' -i {input} -o {params.stats_path} -k {wildcards.k} -m {wildcards.m} --silent2',

    # infers k and m values.
    'pa_inf': binaries["pa"]+' -i {input} --error-rate {wildcards.e} --silent2 ',

    # slower PA variants
    'dijkstra': binaries["pa"]+' -i {input} -a Dijkstra --silent2',
    'pa_noprune': binaries["pa"]+' -i {input} --error-rate {wildcards.e} --silent2 --no-prune',

    # Others
    'edlib': binaries["edlib"]+' {input} -p -s',
    'wfa': binaries["wfa"]+' -i {input} -a gap-affine-wfa --affine-penalties="0,1,0,1"',
    'wfa2': binaries["wfa2"]+' -i {input} -a edit-wfa',
    'biwfa': binaries["biwfa"]+' -i {input} --affine-penalties 0,1,0,1 --wfa-bidirectional',
}

# Subset of tools to run
tools = [
    'pa_inf',
    'dijkstra',
    'pa_noprune',
    'edlib',
    'biwfa',
]

# Generate testcases
GENERATE_CMD    = '../target/release/examples/generate_dataset -x {wildcards.cnt} -n {wildcards.n} -e {wildcards.e} -o {output}'

wildcard_constraints:
    tool='[^-]*'

# Wrap a comman in a small shell script than handles timeouts.
# Uses the TIMEOUT variable, and writes status to params.status_path.
def timeout(command):
    return f"""
# Allow this command to fail.
set +e
timeout {{TIMEOUT}} {command}
exit_code=$?
set -e
# Normal exit
if [ $exit_code -eq 0 ] ; then
    echo ok > {{params.status_path}}
    exit 0
fi
# Handle timeout
if [ $exit_code -eq 124 ] ; then
    echo timeout {{TIMEOUT}} > {{params.status_path}}
    exit 0
fi
# Handle out of memory (OOM);
# typically from SIGKILL (137) (when killed by linux) or SIGSEGV (139) or SIGABRT (134)
if [ $exit_code -eq 137 ] || [ $exit_code -eq 139 ] || [ $exit_code -eq 134 ]; then
    echo out-of-memory $exit_code > {{params.status_path}}
    exit 0
fi
echo Unexpected exit status $exit_code
# Fail with an error.
exit 1
"""


## WRAPPER CLASSES
wildcard_constraints:
    e="[0-9.]+"

class Input:
    def __init__(self, **kwargs):
        self.cnt = int(kwargs['cnt'])
        self.n = int(kwargs['n'])
        self.e = float(kwargs['e'])
    def name_pattern():
        return 'x{cnt}-n{n}-e{e}'
    def pattern():
        return 'input/synthetic/x{cnt}-n{n}-e{e}.seq'
    def name(self):
        return Path(f'x{self.cnt}-n{self.n}-e{self.e}')
    def path(self):
        return Path(f'input/synthetic/{self.name()}.seq')

def inputs(N):
    N = int(float(N))
    return [Input(cnt=N//n, n=n, e=e) for n in ns for e in es if N//n > 0]

class Run:
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)
    def pattern_with_params():
        tool = 'pa'
        return f'runs/{Input.name_pattern()}-A{tool}-m{{m}}-k{{k}}.bench'
    def pattern(tool=None):
        if tool:
            return f'runs/{Input.name_pattern()}-A{tool}.bench'
        else:
            return f'runs/{Input.name_pattern()}-A{{tool}}.bench'
    def bench_path_with_params(self):
        assert self.tool == "pa"
        return Path(f'runs/{self.input.name()}-A{self.tool}-m{self.m}-k{self.k}.bench')
    def stats_path_with_params(self):
        return self.bench_path_with_params().with_suffix('.stats')
    def status_path_with_params(self):
        return self.bench_path_with_params().with_suffix('.status')
    def bench_path(self):
        return Path(f'runs/{self.input.name()}-A{self.tool}.bench')
    def status_path(self):
        return self.bench_path().with_suffix('.status')

# Returns false for parameters that should be skipped.
def run_filter(run):
    if run.tool != 'pa': return True
    # If the error rate is larger than the max rate this (k,m) can handle, skip.
    # e is made a bit lower, since an induced error rate of 20% only gives ~15% edit distance.
    max_error_rate = (run.m+1) / (run.k-1) # -1 to slightly shift the boundary
    if run.input.e * 0.85 > max_error_rate:
        return False

    # Don't do inexact matches for low error rates.
    if run.input.e < 0.10 and run.m > 0:
        return False

    # If we expect multiple matches per seed, skip since we should increase k instead.
    if run.m == 0 and run.input.n / 4.**(run.k+1) > 2*(run.k+1): # +1 to slightly shift the boundary
        return False
    # The number of distance-1 matches per seed is roughly 8*k.
    if run.m == 1 and 8*(run.k+1)* run.input.n / 4.**(run.k+1) > 2*(run.k+1): # +1 to slightly shift the boundary
        return False

    return True

def params_runs(N):
    return list(filter(run_filter, (Run(input=input, tool='pa', m=m, k=k)
                        for input in inputs(N)
                        for (k, m) in kms)))

def params_runs_for_input(input):
    return list(filter(run_filter, (Run(input=input, tool='pa', m=m, k=k)
                        for (k, m) in kms)))

def tool_runs(N):
    return [Run(input=input, tool=tool)
                for input in inputs(N)
                for tool in tools]

## INPUT DATA RULES

# Nanopore / Illumina data is here: https://github.com/Daniel-Liu-c0deb0t/block-aligner/releases

# Synthetic tests
rule generate_input:
    output: Input.pattern()
    shell: GENERATE_CMD

# ONT long reads
# Block aligner uses data from the
# Whole Human Genome Sequencing Project
# Data sources: https://github.com/nanopore-wgs-consortium/NA12878/blob/master/nanopore-human-genome/rel_3_4.md

# Download reference genome.
# Gets the entire directory, but not subdirectories.
REFERENCE_FTP = 'ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/GRCh38_reference_genome/'
REFERENCE_DIR = 'input/reads/reference'
REFERENCE_NAME = f'{REFERENCE_DIR}/GRCh38_full_analysis_set_plus_decoy_hla.fa'
rule download_reference:
    output: REFERENCE_NAME
    shell: f'wget -r -nH -nc -l 1 -P {REFERENCE_DIR} --cut-dirs=5 {REFERENCE_FTP}'


# Download reads
# [(flowcel id, fasta url, bam url, bai url)]
FLOWCELLS = {
    'FAB45271': 'nanopore-human-wgs/rel3-nanopore-wgs-152889212-FAB45271.fastq.gz',
    'FAB42316': 'nanopore-human-wgs/rel3-nanopore-wgs-216722908-FAB42316.fastq.gz',
    'FAB49164': 'nanopore-human-wgs/rel3-nanopore-wgs-4045668814-FAB49164.fastq.gz',
}

# Copy from AWS S3.
# Using AWS CLI is faster, but needs an AWS account and aws CLI set up and linked to your account.
rule read:
    output: 'input/reads/{id}.fastq.gz'
    run:
        path = FLOWCELLS[wildcards.id]
        #shell('wget -O {output} http://s3.amazonaws.com/{path}')
        shell('aws configure set default.s3.max_concurrent_requests 20')
        shell('aws s3 cp s3://{path} {output}')

rule read_bam:
    output: 'input/reads/{id}.fastq.gz.sorted.bam'
    run:
        path = FLOWCELLS[wildcards.id] + '.sorted.bam'
        for ext in ['', '.bai']:
            #shell('wget -O {output}{ext} http://s3.amazonaws.com/{path}{ext}')
            shell('aws configure set default.s3.max_concurrent_requests 20')
            shell('aws s3 cp s3://{path}{ext} {output}{ext}')

rule download_reads:
    input: [f'input/reads/{id}.fastq.gz{ext}' for ext in ['', '.sorted.bam'] for id in FLOWCELLS]

# Make .bed file containing reference locations of reads in .bam file:
rule bed_file:
    input: 'input/reads/{id}.fastq.gz.sorted.bam'
    output: 'input/reads/{id}.bed'
    shell: 'bedtools bamtobed -i input/reads{id}.fastq.gz.sorted.bam > {output}'

# Extract parts of reference corresponding to reads:
rule extract_reference:
    input: ['input/reads/{id}.bed', REFERENCE_NAME]
    output: 'input/reads/{id}.mapped.fa'
    # -s: reverse-complement the reference when needed
    shell: 'bedtools getfasta -s -nameOnly -fi {REFERENCE_NAME}  -bed input/reads/{id}.bed > {output}'

# Reads are in the original .fasta.gz file, or can be extracted using
# samtools fasta {id}.fastq.gz.sorted.bam
# View the bam entry for a read with
# samtools view {read_id}


## PA WITH PARAMS

rule run_pairwise_aligner:
    input: lambda w: Input(**w).path()
    benchmark: repeat(Run.pattern_with_params(), REPEATS)
    params:
        stats_path = lambda w: Run(input=Input(**w), **w, tool='pa').stats_path_with_params(),
        status_path = lambda w: Run(input=Input(**w), **w, tool='pa').status_path_with_params()
    shell: timeout(tool_command['pa'])

# Concatenate the PA output to the Snakemake bench file.
def params_table(runs, output):
    headers = ["alg","cnt","n","e", "m", "k", "s","h:m:s","max_rss","max_vms","max_uss","max_pss","io_in","io_out","mean_load","cpu_time", "exit_status"]
    table_file = Path(output[0]).open('w')

    found_headers = False
    for run in runs:
        if run.stats_path_with_params().is_file():
            pa_headers = run.stats_path_with_params().read_text().splitlines()[0]
            table_file.write("\t".join(headers) + '\t' + pa_headers + '\n')
            found_headers = True
            break
    assert found_headers

    for run in runs:
        status = run.status_path_with_params().read_text().strip()
        try:
            stats = run.stats_path_with_params().read_text().splitlines()[1]
        except:
            stats = ''
        for bench_line in run.bench_path_with_params().read_text().splitlines()[1:]:
            table_file.write(f'{run.tool}\t{run.input.cnt}\t{run.input.n}\t{run.input.e}\t{run.m}\t{run.k}\t{bench_line}\t{status}\t{stats}\n')

# Collect all .bench files into a single tsv.
rule params_table:
    input: lambda w: [run.bench_path_with_params() for run in params_runs(**w)]
    output: 'table/params_N{N}.tsv'
    run: params_table(params_runs(**wildcards), output)

## RUN OTHER TOOLS

rule run_tool:
    input: lambda w: Input(**w).path()
    benchmark: repeat(Run.pattern(), REPEATS)
    params: status_path = lambda w: Run(input=Input(**w), **w).status_path()
    run: shell(timeout(tool_command[wildcards.tool]))

# Collect all .bench files into a single tsv.
rule tools_table:
    input: lambda w: [run.bench_path() for run in tool_runs(**w)]
    output: 'table/tools_N{N}.tsv'
    run:
        headers = ["alg","cnt","n","e","s","h:m:s","max_rss","max_vms","max_uss","max_pss","io_in","io_out","mean_load","cpu_time", "exit_status"]
        table_file = Path(output[0]).open('w')
        table_file.write("\t".join(headers) + '\n')
        for run in tool_runs(**wildcards):
            lines = run.bench_path().read_text().splitlines()
            status = run.status_path().read_text().strip()
            for bench_line in lines[1:]:
                table_file.write(f'{run.tool}\t{run.input.cnt}\t{run.input.n}\t{run.input.e}\t{bench_line}\t{status}\n')

# Visualizations

rule astar_visualization:
    input:
    shell: "cargo run --release --example states"
